{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekharsomani/anaconda3/envs/melotts/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/shekharsomani/anaconda3/envs/melotts/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from melo.api import TTS\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scratchpad': \"To make the topic engaging, we can use an analogy of a detective solving a complex case. The detective (Data Interpreter) receives a mysterious dataset (the crime scene) and must piece together clues (data points) to solve the case (derive insights). We can also use storytelling techniques to illustrate how Data Interpreter handles different types of data science tasks, such as data preprocessing, feature engineering, and model training. For complex topics, we can break them down into simpler parts and use relatable examples. For instance, we can compare feature engineering to preparing ingredients for a recipe, where each ingredient (feature) contributes to the final dish (model performance). To make the discussion more interactive, we can ask thought-provoking questions, such as 'How does Data Interpreter handle missing data in a dataset?' or 'What strategies does Data Interpreter use to improve model accuracy?' We can also include light-hearted moments, such as imagining the detective struggling to decode a particularly tricky piece of evidence (a complex data point). To fill any gaps in the information, we can use hypothetical scenarios or examples to illustrate potential challenges and solutions.\", 'name_of_guest': 'Sirui Hong', 'dialogue': [{'speaker': 'Jane', 'text': \"Welcome to the show, Sirui! Today, we're diving into the fascinating world of Data Interpreter, an LLM-based agent designed to tackle complex data science challenges.\"}, {'speaker': 'Sirui', 'text': \"Thanks for having me, Jane. I'm excited to discuss Data Interpreter and how it can revolutionize the way we approach data science tasks.\"}, {'speaker': 'Jane', 'text': \"Let's start with the basics. Can you explain what Data Interpreter is and how it differs from traditional data science methods?\"}, {'speaker': 'Sirui', 'text': 'Absolutely. Data Interpreter is an LLM-based agent that uses hierarchical graph modeling to break down complex data science problems into manageable tasks.'}, {'speaker': 'Jane', 'text': 'That sounds intriguing. How does it handle the dynamic nature of data science workflows, where data and requirements constantly evolve?'}, {'speaker': 'Sirui', 'text': 'Great question. Data Interpreter incorporates iterative graph refinement, allowing it to adapt to changing data and task requirements in real-time.'}, {'speaker': 'Jane', 'text': 'Can you give us an example of how Data Interpreter might approach a typical data science problem, like predicting machine operational status?'}, {'speaker': 'Sirui', 'text': 'Sure. Imagine we have a dataset with sensor readings from industrial machines. Data Interpreter would start by conducting data exploration to understand the structure and missing values.'}, {'speaker': 'Jane', 'text': 'And then what happens?'}, {'speaker': 'Sirui', 'text': 'Next, it would perform correlation analysis to identify relationships between variables, followed by anomaly detection to handle outliers.'}, {'speaker': 'Jane', 'text': \"That's impressive. How does Data Interpreter ensure the accuracy and robustness of its solutions?\"}, {'speaker': 'Sirui', 'text': 'Data Interpreter uses programmable node generation, which refines and verifies each subproblem iteratively, improving code generation results and robustness.'}, {'speaker': 'Jane', 'text': 'What about more complex tasks, like feature engineering or model training? How does Data Interpreter handle those?'}, {'speaker': 'Sirui', 'text': 'For feature engineering, Data Interpreter uses techniques like target mean encoding and polynomial expansion to prepare the dataset for predictive modeling.'}, {'speaker': 'Jane', 'text': 'And for model training?'}, {'speaker': 'Sirui', 'text': 'Data Interpreter develops a predictive model using the processed dataset and evaluates its performance using techniques like cross-validation.'}, {'speaker': 'Jane', 'text': 'How does Data Interpreter compare to other open-source frameworks in terms of performance and efficiency?'}, {'speaker': 'Sirui', 'text': 'Data Interpreter consistently outperforms existing methods across several benchmarks, achieving significant improvements in accuracy and efficiency.'}, {'speaker': 'Jane', 'text': 'Can you share some of the key insights or takeaways from your experiments with Data Interpreter?'}, {'speaker': 'Sirui', 'text': 'One key insight is the importance of iterative refinement and dynamic adaptation in handling complex data science tasks.'}, {'speaker': 'Jane', 'text': \"That makes a lot of sense. Thank you, Sirui, for sharing your insights on Data Interpreter. It's clear that this tool has the potential to revolutionize the field of data science.\"}, {'speaker': 'Sirui', 'text': \"You're welcome, Jane. It's been a pleasure discussing Data Interpreter with you.\"}, {'speaker': 'Jane', 'text': \"That's all for today's episode. Thanks for joining us, and stay tuned for more exciting discussions on the latest in data science and AI.\"}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"podcast_data.json\", \"r\") as file:\n",
    "    dialogue_json = json.load(file)\n",
    "\n",
    "dialogue_json = json.loads(dialogue_json)\n",
    "\n",
    "print(dialogue_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Welcome to the show, Sirui! Today, we're diving into the fascinating world of Data Interpreter, an LLM-based agent designed to tackle complex data science challenges.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/shekharsomani/anaconda3/envs/melotts/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/shekharsomani/anaconda3/envs/melotts/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.34s/it]\n",
      "/Users/shekharsomani/anaconda3/envs/melotts/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Thanks for having me, Jane. I'm excited to discuss Data Interpreter and how it can revolutionize the way we approach data science tasks.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Let's start with the basics. Can you explain what Data Interpreter is and how it differs from traditional data science methods?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Absolutely. Data Interpreter is an LLM-based agent that uses hierarchical graph modeling to break down complex data science problems into manageable tasks.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "That sounds intriguing. How does it handle the dynamic nature of data science workflows, where data and requirements constantly evolve?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Great question. Data Interpreter incorporates iterative graph refinement, allowing it to adapt to changing data and task requirements in real-time.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Can you give us an example of how Data Interpreter might approach a typical data science problem, like predicting machine operational status?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Sure. Imagine we have a dataset with sensor readings from industrial machines. Data Interpreter would start by conducting data exploration to understand the structure and missing values.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "And then what happens?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Next, it would perform correlation analysis to identify relationships between variables, followed by anomaly detection to handle outliers.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "That's impressive. How does Data Interpreter ensure the accuracy and robustness of its solutions?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Data Interpreter uses programmable node generation, which refines and verifies each subproblem iteratively, improving code generation results and robustness.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "What about more complex tasks, like feature engineering or model training? How does Data Interpreter handle those?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "For feature engineering, Data Interpreter uses techniques like target mean encoding and polynomial expansion to prepare the dataset for predictive modeling.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "And for model training?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Data Interpreter develops a predictive model using the processed dataset and evaluates its performance using techniques like cross-validation.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "How does Data Interpreter compare to other open-source frameworks in terms of performance and efficiency?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Data Interpreter consistently outperforms existing methods across several benchmarks, achieving significant improvements in accuracy and efficiency.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "Can you share some of the key insights or takeaways from your experiments with Data Interpreter?\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "One key insight is the importance of iterative refinement and dynamic adaptation in handling complex data science tasks.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "That makes a lot of sense. Thank you, Sirui, for sharing your insights on Data Interpreter. It's clear that this tool has the potential to revolutionize the field of data science.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "You're welcome, Jane. It's been a pleasure discussing Data Interpreter with you.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "That's all for today's episode. Thanks for joining us, and stay tuned for more exciting discussions on the latest in data science and AI.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.52s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_segments = []\n",
    "transcript = \"\"\n",
    "\n",
    "for i, line in enumerate(dialogue_json.get(\"dialogue\", [])):\n",
    "    speaker = line.get(\"speaker\", \"Unknown\")\n",
    "    text = line.get(\"text\", \"\")\n",
    "    # Add to transcript\n",
    "    if speaker == \"Jane\":\n",
    "        # print(f\"**Host**: {text}\\n\\n\")\n",
    "        transcript += f\"**Host**: {text}\\n\\n\"\n",
    "\n",
    "    else:\n",
    "        # print(f\"**{dialogue_json.get('name_of_guest', 'Guest')}**: {text}\\n\\n\")\n",
    "        transcript += f\"**{dialogue_json.get('name_of_guest', 'Guest')}**: {text}\\n\\n\"\n",
    "\n",
    "    model = TTS(language='EN', device=\"cpu\")\n",
    "    if speaker == \"Jane\":\n",
    "        speaker_id = model.hps.data.spk2id['EN-Default']  # Female voice for host\n",
    "    else:\n",
    "        speaker_id = model.hps.data.spk2id['EN-BR']  # Male voice for guest\n",
    "\n",
    "    temp_file = f\"podcast/segment_{i}.mp3\"\n",
    "    model.tts_to_file(\n",
    "    text=text,\n",
    "    speaker_id=speaker_id,\n",
    "    output_path=temp_file,\n",
    "    sdp_ratio=0.2,       # Attention control parameter\n",
    "    noise_scale=0.6,     # Noise for variance adaptor\n",
    "    noise_scale_w=0.8,   # Noise for duration predictor\n",
    "    speed=1.0            # Speech speed\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "podcast/segment_0.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_1.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_2.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_3.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_4.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_5.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_6.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_7.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_8.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_9.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_10.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_11.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_12.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_13.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_14.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_15.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_16.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_17.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_18.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_19.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_20.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_21.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "podcast/segment_22.mp3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "for i, line in enumerate(dialogue_json.get(\"dialogue\", [])):\n",
    "    temp_file = f\"podcast/segment_{i}.mp3\"\n",
    "    print(temp_file)\n",
    "    segment = AudioSegment.from_file(temp_file)\n",
    "    audio_segments.append(segment)\n",
    "    # Add a short pause between segments\n",
    "    pause = AudioSegment.silent(duration=500)  # 500ms pause\n",
    "    audio_segments.append(pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m combined_audio\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mstr\u001b[39m(podcast_path), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save transcript\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m transcript_path \u001b[38;5;241m=\u001b[39m \u001b[43moutput_dir\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscript.md\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(transcript_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(transcript)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "if audio_segments:\n",
    "    combined_audio = sum(audio_segments)\n",
    "    podcast_path =  \"podcast/podcast.mp3\"\n",
    "    combined_audio.export(str(podcast_path), format=\"mp3\")\n",
    "    \n",
    "    # Save transcript\n",
    "    transcript_path =  \"podcast/transcript.md\"\n",
    "    with open(transcript_path, \"w\") as f:\n",
    "        f.write(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekharsomani/anaconda3/envs/melotts/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "model = TTS(language='EN', device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = model.hps.data.spk2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EN-US': 0, 'EN-BR': 1, 'EN_INDIA': 2, 'EN-AU': 3, 'EN-Default': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Host**: Welcome to the show, Sirui! Today, we're diving into the fascinating world of Data Interpreter, an LLM-based agent designed to tackle complex data science challenges.\\n\\n**Sirui Hong**: Thanks for having me, Jane. I'm excited to discuss Data Interpreter and how it can revolutionize the way we approach data science tasks.\\n\\n**Host**: Let's start with the basics. Can you explain what Data Interpreter is and how it differs from traditional data science methods?\\n\\n**Sirui Hong**: Absolutely. Data Interpreter is an LLM-based agent that uses hierarchical graph modeling to break down complex data science problems into manageable tasks.\\n\\n**Host**: That sounds intriguing. How does it handle the dynamic nature of data science workflows, where data and requirements constantly evolve?\\n\\n**Sirui Hong**: Great question. Data Interpreter incorporates iterative graph refinement, allowing it to adapt to changing data and task requirements in real-time.\\n\\n**Host**: Can you give us an example of how Data Interpreter might approach a typical data science problem, like predicting machine operational status?\\n\\n**Sirui Hong**: Sure. Imagine we have a dataset with sensor readings from industrial machines. Data Interpreter would start by conducting data exploration to understand the structure and missing values.\\n\\n**Host**: And then what happens?\\n\\n**Sirui Hong**: Next, it would perform correlation analysis to identify relationships between variables, followed by anomaly detection to handle outliers.\\n\\n**Host**: That's impressive. How does Data Interpreter ensure the accuracy and robustness of its solutions?\\n\\n**Sirui Hong**: Data Interpreter uses programmable node generation, which refines and verifies each subproblem iteratively, improving code generation results and robustness.\\n\\n**Host**: What about more complex tasks, like feature engineering or model training? How does Data Interpreter handle those?\\n\\n**Sirui Hong**: For feature engineering, Data Interpreter uses techniques like target mean encoding and polynomial expansion to prepare the dataset for predictive modeling.\\n\\n**Host**: And for model training?\\n\\n**Sirui Hong**: Data Interpreter develops a predictive model using the processed dataset and evaluates its performance using techniques like cross-validation.\\n\\n**Host**: How does Data Interpreter compare to other open-source frameworks in terms of performance and efficiency?\\n\\n**Sirui Hong**: Data Interpreter consistently outperforms existing methods across several benchmarks, achieving significant improvements in accuracy and efficiency.\\n\\n**Host**: Can you share some of the key insights or takeaways from your experiments with Data Interpreter?\\n\\n**Sirui Hong**: One key insight is the importance of iterative refinement and dynamic adaptation in handling complex data science tasks.\\n\\n**Host**: That makes a lot of sense. Thank you, Sirui, for sharing your insights on Data Interpreter. It's clear that this tool has the potential to revolutionize the field of data science.\\n\\n**Sirui Hong**: You're welcome, Jane. It's been a pleasure discussing Data Interpreter with you.\\n\\n**Host**: That's all for today's episode. Thanks for joining us, and stay tuned for more exciting discussions on the latest in data science and AI.\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segments = []\n",
    "transcript = \"\"\n",
    "\n",
    "for i, line in enumerate(dialogue_json.get(\"dialogue\", [])):\n",
    "    speaker = line.get(\"speaker\", \"Unknown\")\n",
    "    text = line.get(\"text\", \"\")\n",
    "    # Add to transcript\n",
    "    if speaker == \"Jane\":\n",
    "        # print(f\"**Host**: {text}\\n\\n\")\n",
    "        transcript += f\"**Host**: {text}\\n\\n\"\n",
    "    else:\n",
    "        # print(f\"**{dialogue_json.get('name_of_guest', 'Guest')}**: {text}\\n\\n\")\n",
    "        transcript += f\"**{dialogue_json.get('name_of_guest', 'Guest')}**: {text}\\n\\n\"\n",
    "\n",
    "    # Set different voices for host and guest if available\n",
    "    voices = engine.getProperty('voices')\n",
    "    if len(voices) > 1:\n",
    "        if speaker == \"Jane\":\n",
    "            engine.setProperty('voice', voices[1].id)  # Female voice for host\n",
    "        else:\n",
    "            engine.setProperty('voice', voices[0].id)  # Male voice for guest\n",
    "            \n",
    "        # Save audio segment\n",
    "        temp_file = output_dir / f\"segment_{i}.mp3\"\n",
    "        engine.save_to_file(text, str(temp_file))\n",
    "        engine.runAndWait()\n",
    "\n",
    "        # Read audio segment\n",
    "        segment = AudioSegment.from_file(temp_file)\n",
    "        audio_segments.append(segment)\n",
    "        # Add a short pause between segments\n",
    "        pause = AudioSegment.silent(duration=500)  # 500ms pause\n",
    "        audio_segments.append(pause)\n",
    "      # Combine audio segments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melotts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
